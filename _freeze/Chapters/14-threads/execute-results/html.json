{
  "hash": "5330e6bae2255d3ad46893b0641f0683",
  "result": {
    "engine": "knitr",
    "markdown": "---\nengine: knitr\nknitr: true\nsyntax-definition: \"../Assets/zig.xml\"\n---\n\n\n\n\n\n\n\n\n# Introducing threads and parallelism in Zig {#sec-thread}\n\nThreads are available in Zig through the `Thread` struct from the Zig Standard Library.\nThis struct represents a kernel thread, and it follows a POSIX Thread pattern,\nmeaning that, it works similarly to a thread from the `pthread` C library, which is usually available on any distribution\nof the GNU C Compiler (`gcc`). If you are not familiar with threads, I will give you some theory behind it first, shall we?\n\n\n## What are threads? {#sec-what-thread}\n\nA thread is basically a separate context of execution.\nWe use threads to introduce parallelism into our program,\nwhich in most cases, makes the program run faster, because we have multiple tasks\nbeing performed at the same time, parallel to each other.\n\nPrograms are normally single-threaded by default. Which means that each program\nusually runs on a single thread, or, a single context of execution. When we have only one thread running, we have no\nparallelism. And when we don't have parallelism, the commands are executed sequentially, that is,\nonly one command is executed at a time, one after another. By creating multiple threads inside our program,\nwe start to execute multiple commands at the same time.\n\nPrograms that create multiple threads are very common in the wild. Because many different types\nof applications are well suited for parallelism. Good examples are video and photo-editing applications\n(e.g. Adobe Photoshop or DaVinci Resolve), games (e.g. The Witcher 3), and also web browsers\n(e.g. Google Chrome, Firefox, Microsoft Edge, etc).\nFor example, in web browsers, threads are normally used to implement tabs.\nThe tabs in a web browsers usually run as separate threads in the main process of\nthe web browser. That is, each new tab that you open in your web browser\nusually runs on a separate thread of execution.\n\nBy running each tab in a separate thread, we allow all open tabs in the browser to run at the same time,\nand independently from each other. For example, you might have YouTube or Spotify currently open in\na tab, and you are listening to some podcast in that tab while at the same time\nworking in another tab, writing an essay on Google Docs. Even if you are not looking\ninto the YouTube tab, you can still hear the podcast only because this YouTube tab is running in parallel\nwith the other tab where Google Docs is running.\n\nWithout threads, the other alternative would be to run each tab as a completely separate\nprocess in your computer. But that would be a bad choice because just a few tabs would already consume\ntoo much power and resources from your computer. In other words, it's very expensive to create a completely new process,\ncompared to creating a new thread of execution. Also, the chances of you experiencing lag and overhead\nwhile using the browser would be significant. Threads are faster to create, and they also consume\nmuch, much less resources from the computer, especially because they share some resources\nwith the main process.\n\nTherefore, it's the use of threads in modern web browsers that allow you to hear the podcast\nat the same time while you are writing something on Google Docs.\nWithout threads, a web browser would probably be limited to just one single tab.\n\nThreads are also well-suited for anything that involves serving requests or orders.\nBecause serving a request takes time, and usually involves a lot of \"waiting time\".\nIn other words, we spend a lot of time in idle, waiting for something to complete.\nFor example, consider a restaurant. Serving orders in a restaurant usually involves\nthe following steps:\n\n1. receive order from the client.\n1. pass the order to the kitchen, and wait for the food to be cooked.\n1. start cooking the food in the kitchen.\n1. when the food is fully cooked deliver this food to the client.\n\nIf you think about the bullet points above, you will notice that one big moment of waiting time\nis present in this whole process, which is while the food is being cooked\ninside the kitchen. While the food is being prepped, both the waiter and the client\nthemselves are waiting for the food to be ready and delivered.\n\nIf we write a program to represent this restaurant, more specifically, a single-threaded program, then\nthis program would be very inefficient. Because the program would stay in idle, waiting for a considerable amount\nof time on the \"check if food is ready\" step. Consider the code snippet exposed below that could\npotentially represent such program.\n\nThe problem with this program is the while loop. This program will spend a lot of time\nwaiting on the while loop, doing nothing more than just checking if the food is ready.\nThis is a waste of time. Instead of waiting for something to happen, the waiter\ncould just send the order to the kitchen, and just move on, and continue with receiving\nmore orders from other clients, and sending more orders to the kitchen, instead\nof doing nothing and waiting for the food to be ready.\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst order = Order.init(\"Pizza Margherita\", n = 1);\nconst waiter = Waiter.init();\nwaiter.receive_order(order);\nwaiter.ask_kitchen_to_cook();\nvar food_not_ready = true;\nwhile (food_not_ready) {\n    food_not_ready = waiter.is_food_ready();\n}\nconst food = waiter.get_food_from_kitchen();\nwaiter.send_food_to_client(food);\n```\n:::\n\n\n\n\nThis is why threads would be a great fit for this program. We could use threads\nto free the waiters from their \"waiting duties\", so they can go on with their\nother tasks, and receive more orders. Take a look at the next example, where I have re-written the above\nprogram into a different program that uses threads to cook and deliver the orders.\n\nYou can see in this program that when a waiter receives a new order\nfrom a client, this waiter executes the `send_order()` function.\nThe only thing that this function does is to create a new thread\nand detaches it. Since creating a thread is a very fast operation,\nthis `send_order()` function returns almost immediately,\nso the waiter spends almost no time worrying about the order, and just\nmove on and tries to get the next order from the clients.\n\nInside the new thread created, the order gets cooked by a chef, and when the\nfood is ready, it's delivered to the client's table.\n\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nfn cook_and_deliver_order(order: *Order) void {\n    const chef = Chef.init();\n    const food = chef.cook(order.*);\n    chef.deliver_food(food);\n}\nfn send_order(order: Order) void {\n    const cook_thread = Thread.spawn(\n        .{}, cook_and_deliver_order, .{&order}\n    );\n    cook_thread.detach();\n}\n\nconst waiter = Waiter.init();\nwhile (true) {\n    const order = waiter.get_new_order();\n    if (order) {\n        send_order(order);\n    }\n}\n```\n:::\n\n\n\n\n\n\n## Threads versus processes\n\nWhen we run a program, this program is executed as a *process* in the operating system.\nThis is a one to one relationship, each program or application that you execute\nis a separate process in the operating system. But each program, or each process,\ncan create and contain multiple threads inside of it. Therefore,\nprocesses and threads have a one to many relationship.\n\nThis also means that every thread that we create is always associated with a particular process in our computer.\nIn other words, a thread is always a subset (or a children) of an existing process.\nAll threads share some of the resources associated with the process from which they were created.\nAnd because threads share resources with the process, they are very good for making communication\nbetween tasks easier.\n\nFor example, suppose that you were developing a big and complex application\nthat would be much simpler if you could split it in two, and make these two separate pieces talk\nwith each other. Some programmers opt to effectively write these two pieces of the codebase as two\ncompletely separate programs, and then, they use IPC (*inter-process communication*) to make these\ntwo separate programs/processes talk to each other, and make them work together.\n\nHowever, some programmers find IPC hard to deal with, and, as consequence,\nthey prefer to write one piece of the codebase as the \"main part of the program\",\nor, as the part of the code that runs as the process in the operating system,\nwhile the other piece of the codebase is written as a task to be executed in\na new thread. A process and a thread can easily comunicate with each other\nthrough both control flow, and also, through data, because they share and have\naccess to the same standard file descriptors (`stdout`, `stdin`, `stderr`), and also to the\nsame memory space on the heap and global data section.\n\n\nIn more details, each thread that you create have a separate stack frame reserved just for that thread,\nwhich essentially means that each local object that you create inside this thread, is local to that\nthread, i.e., the other threads cannot see this local object. Unless this object that you have created\nis an object that lives on the heap. In other words, if the memory associated with this object\nis on the heap, then, the other threads can potentially access this object.\n\nTherefore, objects that are stored in the stack are local to the thread where they were created.\nBut objects that are stored on the heap are potentially accessible to other threads. All of this means that,\neach thread has its own separate stack frame, but, at the same time, all threads share\nthe same heap, the same standard file descriptors (which means that they share the same `stdout`, `stdin`, `stderr`),\nand the same global data section in the program.\n\n\n\n## Creating a thread\n\nWe create new threads in Zig by first importing the `Thread` struct into\nour current Zig module and then calling the `spawn()` method of this struct,\nwhich creates (or \"spawns\") a new thread of execution from our current process.\nThis method has three arguments, which are, respectively:\n\n1. a `SpawnConfig` object, which contains configurations for the spawn process.\n1. the name of the function that is going to be executed (or that is going to be \"called\") inside this new thread.\n1. a list of arguments (or inputs) to be passed to the function provided in the second argument.\n\nWith these three arguments, you can control how the thread gets created, and also, specify which\nwork (or \"tasks\") will be performed inside this new thread. A thread is just a separate context of execution,\nand we usually create new threads in our code because we want to perform some work inside this\nnew context of execution. And we specify which exact work, or which exact steps that are going to be\nperformed inside this context by providing the name of a function as the second argument of the `spawn()` method.\n\nThus, when this new thread gets created, this function that you provided as input to the `spawn()`\nmethod gets called, or gets executed inside this new thread. You can control the\narguments, or the inputs that are passed to this function when it gets called by providing\na list of arguments (or a list of inputs) in the third argument of the `spawn()` method.\nThese arguments are passed to the function in the same order that they are\nprovided to `spawn()`.\n\nFurthermore, the `SpawnConfig` is a struct object with only two possible fields, or, two possible members, that you\ncan set to tailor the spawn behaviour. These fields are:\n\n- `stack_size`: you can provide a `usize` value to specify the size (in bytes) of the thread's stack frame. By default, this value is: $16 \\times 1024 \\times 1024$.\n- `allocator`: you can provide an allocator object to be used when allocating memory for the thread.\n\nTo use one of these two fields (or \"configs\"), you just have to create a new object of type `SpawnConfig`,\nand provide this object as input to the `spawn()` method. But, if you are not interested in using\none of these configs, and you are ok with using just the defaults, you can just provide an anonymous\nstruct literal (`.{}`) in place of this `SpawnConfig` argument.\n\nAs our first, and very simple example, consider the code exposed below.\nInside the same program, you can create multiple threads of execution if you want to.\nBut, in this first example, we are creating just a single thread of execution, because\nwe call `spawn()` only once.\n\nAlso, notice in this example that we are executing the function `do_some_work()`\ninside the new thread. Since this function receives no inputs, because it has\nno arguments, we have passed an empty list in this instance, or more precisely,\nan empty, anonymous struct (`.{}`) in the third argument of `spawn()`.\n\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst std = @import(\"std\");\nconst stdout = std.io.getStdOut().writer();\nconst Thread = std.Thread;\nfn do_some_work() !void {\n    _ = try stdout.write(\"Starting the work.\\n\");\n    std.time.sleep(100 * std.time.ns_per_ms);\n    _ = try stdout.write(\"Finishing the work.\\n\");\n}\n\npub fn main() !void {\n    const thread = try Thread.spawn(.{}, do_some_work, .{});\n    thread.join();\n}\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStarting the work.Finishing the work.\n```\n\n\n:::\n:::\n\n\n\n\nNotice the use of `try` when calling the `spawn()` method. This means\nthat this method can return an error in some circumstances. One circumstance\nin particular is when you attempt to create a new thread, when you have already\ncreated too much (i.e., you have exceeded the quota of concurrent threads in your system).\n\nBut, if the new thread is successfully created, the `spawn()` method returns a handler\nobject (which is just an object of type `Thread`) to this new thread. You can use\nthis handler object to effectively control all aspects of the thread.\n\nWhen the thread gets created, the function that you provided as input to `spawn()`\ngets invoked (i.e., gets called) to start the execution on this new thread.\nIn other words, every time you call `spawn()`, not only is a new thread created,\nbut the \"start work button\" of this thread is also automatically pressed.\nSo the work being performed in this thread starts as soon as the thread is created.\nThis is similar to how `pthread_create()` from the `pthreads` library in C works,\nwhich also starts the execution as soon as the thread is created.\n\n\n## Returning from a thread\n\nWe have learned in the previous section that the execution of the thread starts as soon as\nthe thread is created. Now, we will learn how to \"join\" or \"detach\" a thread in Zig.\n\"Join\" and \"detach\" are operations that control how the thread returns to\nthe main thread, or to the main process in our program.\n\nWe perform these operations by using the methods `join()` and `detach()` from the thread handler object.\nEvery thread that you create can be marked as either *joinable* or *detached* [@linux_pthread_create].\nYou can turn a thread into a *detached* thread by calling the `detach()` method\nfrom the thread handler object. But if you call the `join()` method instead, then this thread\nbecomes a *joinable* thread.\n\nA thread cannot be both *joinable* and *detached*. Which in general means\nthat you cannot call both `join()` and `detach()` on the same thread.\nBut a thread must be one of the two, meaning that, you should always call\neither `join()` or `detach()` over a thread. If you don't call\none of these two methods over your thread, you introduce undefined behaviour into your program,\nwhich is described in @sec-not-call-join-detach.\n\nNow, let's describe what each of these two methods do to your thread.\n\n\n### Joining a thread\n\nWhen you join a thread, you are essentially saying: \"Hey! Could you please wait for the thread to finish,\nbefore you continue with your execution?\". For example, if we come back to our first and simplest example\nof a thread in Zig, we created a single thread inside the `main()` function of our program\nand just called `join()` on this thread at the end. This section of the code example is reproduced below.\n\nBecause we are joining this new thread inside the `main()`'s scope, it means that the\nexecution of the `main()` function is temporarily stopped, to wait for the execution of the thread\nto finish. That is, the execution of `main()` stops temporarily at the line where `join()` gets called,\nand it will continue only after the thread has finished its tasks.\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\npub fn main() !void {\n    const thread = try Thread.spawn(.{}, do_some_work, .{});\n    thread.join();\n}\n```\n:::\n\n\n\n\nBecause we have joined this new thread inside the `main()` scope, we have a\nguarantee that this new thread will finish before the end of the execution of `main()`.\nBecause it's guaranteed that `main()` will wait for the thread to finish its tasks.\n\nIn the example above, there are no more expressions after the `join()` call. We just have the end\nof the `main()`'s scope, and, therefore, the execution of our program just ends after the thread finishes its tasks,\nsince there is nothing more to do. But what if we had more stuff to do after the join call?\n\nTo demonstrate this other possibility, consider the next example exposed\nbelow. Here, we create a `print_id()` function, that just receives an id\nas input, and prints it to `stdout`. In this example, we are creating two\nnew threads, one after another. Then, we join the first thread, then,\nwe wait for two whole seconds, then, at last, we join the second thread.\n\nThe idea behind this example is that the last `join()` call is executed\nonly after the first thread finishes its task (i.e., the first `join()` call),\nand the two-second delay. If you compile and run this\nexample, you will notice that most messages are quickly printed to `stdout`,\ni.e., they appear almost instantly on your screen.\nHowever, the last message (\"Joining thread 2\") takes around 2 seconds to appear\non the screen.\n\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nfn print_id(id: *const u8) !void {\n    try stdout.print(\"Thread ID: {d}\\n\", .{id.*});\n}\n\npub fn main() !void {\n    const id1: u8 = 1;\n    const id2: u8 = 2;\n    const thread1 = try Thread.spawn(.{}, print_id, .{&id1});\n    const thread2 = try Thread.spawn(.{}, print_id, .{&id2});\n\n    _ = try stdout.write(\"Joining thread 1\\n\");\n    thread1.join();\n    std.time.sleep(2 * std.time.ns_per_s);\n    _ = try stdout.write(\"Joining thread 2\\n\");\n    thread2.join();\n}\n```\n:::\n\n\n\n\n```\nThread ID: Joining thread 1\n1\nThread ID: 2\nJoining thread 2\n```\n\nThis demonstrates that both threads finish their work (i.e., printing the IDs)\nvery fast, before the two seconds of delay end. Because of that, the last `join()` call\nreturns pretty much instantly. Because when this last `join()` call happens, the second\nthread has already finished its task.\n\nNow, if you compile and run this example, you will also notice that, in some cases,\nthe messages intertwine with each other. In other words, you might see\nthe message \"Joining thread 1\" inserted in the middle of the message \"Thread 1\",\nor vice-versa. This happens because:\n\n- the threads are executing basically at the same time as the main process of the program (i.e., the `main()` function).\n- the threads share the same `stdout` from the main process of the program, which means that the messages that the threads produce are sent to exact same place as the messages produced by the main process.\n\nBoth of these points were described previously in @sec-what-thread.\nSo the messages might get intertwined because they are being produced and\nsent to the same `stdout` roughly at the same time.\nAnyway, when you call `join()` over a thread, the current process will wait\nfor the thread to finish before it continues, and, when the thread finishes its\ntask, the resources associated with this thread are automatically freed, and\nthe current process continues with its execution.\n\n\n### Detaching a thread\n\nWhen you detach a thread, the resources associated with this thread are automatically\nreleased back to the system, without the need for another thread to join with this terminated thread.\n\nIn other words, when you call `detach()` on a thread it's like when your children become adults,\ni.e., they become independent from you. A detached thread frees itself, and when this thread finishes its\ntasks, it does not report the results back to you. Thus, you normally mark a thread as *detached*\nwhen you don't need to use the return value of the thread, or when you don't care about\nwhen exactly the thread finishes its job, i.e., the thread solves everything by itself.\n\nTake the code example below. We create a new thread, detach it, and then, we just\nprint a final message before we end our program. We use the same `print_id()`\nfunction that we have used over the previous examples.\n\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nfn print_id(id: *const u8) !void {\n    try stdout.print(\"Thread ID: {d}\\n\", .{id.*});\n}\n\npub fn main() !void {\n    const id1: u8 = 1;\n    const thread1 = try Thread.spawn(.{}, print_id, .{&id1});\n    thread1.detach();\n    _ = try stdout.write(\"Finish main\\n\");\n}\n```\n:::\n\n\n\n\n```\nFinish main\n```\n\nNow, if you look closely at the output of this code example, you will notice\nthat only the final message in main was printed to the console. The message\nthat was supposed to be printed by `print_id()` did not appear in the console.\nWhy? It's because the main process of our program has finished first,\nbefore the thread was able to say anything.\n\nAnd that is perfectly ok behaviour, because the thread was detached, so it was\nable to free itself, without the need to wait for the main process.\nIf you ask main to sleep (or \"wait\") for some extra nanoseconds, before it ends, you will likely\nsee the message printed by `print_id()`, because you give enough time for the thread to\nfinish before the main process ends.\n\n\n## Thread pools\n\nThread pools is a very popular programming pattern, which is used especially on servers and daemons processes.\nA thread pool is just a set of threads, or a \"pool\" of threads. Many programmers like to use this pattern because it makes\nit easier to manage and use multiple threads in your program, instead of manually creating the threads when you need them.\n\nAlso, using thread pools might increase performance as well in your program,\nespecially if your program is constantly creating threads to perform short-lived tasks.\nIn such instance, a thread pool might cause an increase in performance because you do not have be constantly\ncreating and destroying threads all the time, so you don't face a lot of the overhead involved\nin this constant process of creating and destroying threads.\n\nThe main idea behind a thread pool is to have a set of threads already created and ready to perform\ntasks at all times. You create a set of threads at the moment that your program starts, and keep\nthese threads alive while your program runs. Each of these threads will be either performing a task, or\nwaiting for a task to be assigned.\nEvery time a new task emerges in your program, this task is added to a \"queue of tasks\",\nand the moment that a thread becomes available and ready to perform a new task,\nthis thread takes the next task from the \"queue of tasks\", and it simply performs the task.\n\nThe Zig Standard Library offers a thread pool implementation on the `std.Thread.Pool` struct.\nYou create a new instance of a `Pool` object by providing a `Pool.Options` object\nas input to the `init()` method of this struct. A `Pool.Options` object, is a struct object that contains\nconfigurations for the pool of threads. The most important settings in this struct object are\nthe members `n_jobs` and `allocator`. As the name suggests, the member `allocator` should receive an allocator object,\nwhile the member `n_jobs` specifies the number of threads to be created and maintained in this pool.\n\nConsider the example exposed below, that demonstrates how can we create a new thread pool object.\nHere, we create a `Pool.Options` object that contains\na general purpose allocator object, and also, the `n_jobs` member was set to 4, which\nmeans that the thread pool will create and use 4 threads.\n\nAlso notice that the `pool` object was initially set to `undefined`. This allow us\nto initially declare the thread pool object, but not properly instantiate the\nunderlying memory of the object. You have to initially declare your thread pool object\nby using `undefined` like this, because the `init()` method of `Pool` needs\nto have an initial pointer to properly instantiate the object.\n\nSo, just remember to create your thread pool object by using `undefined`, and then,\nafter that, you call the `init()` method over the object.\nYou should also not forget to call the `deinit()` method over the thread pool\nobject, once you are done with it, to release the resources allocated for the thread pool. Otherwise, you will\nhave a memory leak in your program.\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst std = @import(\"std\");\nconst Pool = std.Thread.Pool;\npub fn main() !void {\n    var gpa = std.heap.GeneralPurposeAllocator(.{}){};\n    const allocator = gpa.allocator();\n    const opt = Pool.Options{\n        .n_jobs = 4,\n        .allocator = allocator,\n    };\n    var pool: Pool = undefined;\n    try pool.init(opt);\n    defer pool.deinit();\n}\n```\n:::\n\n\n\n\nNow that we know how to create `Pool` objects, we have\nto understand how to assign tasks to be executed by the threads in this pool object.\nTo assign a task to be performed by a thread, we need to call the `spawn()` method\nfrom the thread pool object.\n\nThis `spawn()` method works identical to the `spawn()` method from the\n`Thread` object. The method has almost the same arguments as the previous one,\nmore precisely, we don't have to provide a `SpawnConfig` object in this case.\nBut instead of creating a new thread, this `spawn()` method from\nthe thread pool object just registers a new task in the internal \"queue of tasks\" to be performed,\nand any available thread in the pool will get this task, and it will simply perform the task.\n\nIn the example below, we are using our previous `print_id()` function once again.\nBut you may notice that the `print_id()` function is a little different this time,\nbecause now we are using `catch` instead of `try` in the `print()` call.\nCurrently, the `Pool` struct only supports functions that don't return errors\nas tasks. Thus, when assigning tasks to threads in a thread pool, it is essential to use functions\nthat don't return errors. That is why we are using `catch` here, so that the\n`print_id()` function don't return an error.\n\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nfn print_id(id: *const u8) void {\n    _ = stdout.print(\"Thread ID: {d}\\n\", .{id.*})\n        catch void;\n}\nconst id1: u8 = 1;\nconst id2: u8 = 2;\ntry pool.spawn(print_id, .{&id1});\ntry pool.spawn(print_id, .{&id2});\n```\n:::\n\n\n\n\nThis limitation should probably not exist, and, in fact, it's already on the radar of the\nZig team to fix this issue, and it's being tracked in an [open issue](https://github.com/ziglang/zig/issues/18810)[^issue].\nSo, if you do need to provide a function that might return an error as the task\nto be performed by the threads in the thread pool, then, you are either limited to:\n\n- implementing your own thread pool that does not have this limitation.\n- wait for the Zig team to actually fix this issue.\n\n[^issue]: <https://github.com/ziglang/zig/issues/18810>\n\n\n\n\n## Mutexes\n\nMutexes are a classic component of every thread library. In essence, a mutex is a *Mutually Exclusive Flag*, and this flag\nacts like a type of \"lock\", or as a gate keeper to a particular section of your code. Mutexes are related to thread synchronization,\nmore specifically, they prevent you from having some classic race conditions in your program,\nand, therefore, major bugs and undefined behaviour that are usually difficult to track and understand.\n\nThe main idea behind a mutex is to help us to control the execution of a particular section of the code, and to\nprevent two or more threads from executing this particular section of the code at the same time.\nMany programmers like to compare a mutex to a bathroom door (which typically has a lock).\nWhen a thread locks its own mutex object, it's like if the bathroom door was locked.\nTherefore, other people (in this case, other threads) who want to use the same bathroom at the same time\nmust be patient and simply wait for the current occupant (or thread) to unlock the door and get out of the bathroom.\n\nSome other programmers also like to explain mutexes by using the analogy of \"each person will have their turn to speak\".\nThis is the analogy used in the [*Multithreading Code* video from the Computerphile project](https://www.youtube.com/watch?v=7ENFeb-J75k&ab_channel=Computerphile)[^computerphile].\nImagine if you are in a conversation circle. There is a moderator in this circle, which is the person that decides who\nhas the right to speak at that particular moment. The moderator gives a green card (or some sort of an authorization card) to the person that\nis going to speak, and, as a result, everyone else must be silent and hear this person that has the green card.\nWhen the person finishes talking, they give the green card back to the moderator, and the moderator decides\nwho is going to talk next, and delivers the green card to that person. And the cycle goes on like this.\n\n[^computerphile]: <https://www.youtube.com/watch?v=7ENFeb-J75k&ab_channel=Computerphile>\n\n\nA mutex acts like the moderator in this conversation circle. The mutex authorizes one single thread to execute a specific section of the code,\nand it also blocks the other threads from executing this same section of the code. If these other threads want to execute this same\npiece of the code, they are forced to wait for the the authorized thread to finish first.\nWhen the authorized thread finishes executing this code, the mutex authorizes the next thread to execute this code,\nwhile the remaining threads remain blocked. Therefore, a mutex is like a moderator that does a \"each\nthread will have their turn to execute this section of the code\" type of control.\n\n\nMutexes are especially used to prevent data race problems from happening. A data race problem happens when two or more threads\nare trying to read from or write to the same shared object at the same time.\nSo, when you have an object that is shared will all threads, and, you want to avoid two or more threads from\naccessing this same object at the same time, you can use a mutex to lock the part of the code that access this specific object.\nWhen a thread tries to run this code that is locked by a mutex, this thread stops its execution,\nand patiently waits for this section of the codebase to be unlocked to continue.\n\nNotice that mutexes are normally used to lock areas of the codebase that access/modify data that is **shared** with all threads,\ni.e., objects that are either stored in the global data section, or in the heap space of your program.\nSo mutexes are not normally used on areas of the codebase that access/modify objects that are local to the thread.\n\n\n\n### Critical section {#sec-critical-section}\n\nCritical section is a concept commonly associated with mutexes and thread synchronization.\nIn essence, a critical section is the section of the program that a thread access/modify a shared resource\n(i.e., an object, a file descriptor, something that all threads have access to). In other words,\na critical section is the section of the program where race conditions might happen, and, therefore,\nwhere undefined behaviour can be introduced into the program.\n\nWhen we use mutexes in our program, the critical section defines the area of the codebase that we want to lock.\nSo we normally lock the mutex object at the beginning of the critical section,\nand then, we unlock it at the end of the critical section.\nThe two bullet points exposed below comes from the \"Critical Section\" article from GeekFromGeeks,\nand they summarise well the role that a critical section plays in the thread synchronization problem [@geeks_critical_section].\n\n\n1. The critical section must be executed as an atomic operation, which means that once one thread or process has entered the critical section, all other threads or processes must wait until the executing thread or process exits the critical section. The purpose of synchronization mechanisms is to ensure that only one thread or process can execute the critical section at a time.\n1. The concept of a critical section is central to synchronization in computer systems, as it is necessary to ensure that multiple threads or processes can execute concurrently without interfering with each other. Various synchronization mechanisms such as semaphores, mutexes, monitors, and condition variables are used to implement critical sections and ensure that shared resources are accessed in a mutually exclusive manner.\n\n\n### Atomic operations {#sec-atomic-operation}\n\nYou will also see the term \"atomic operation\" a lot when reading about threads, race conditions and mutexes.\nIn summary, an operation is categorized as \"atomic\" when a context switch cannot occur in\nthe middle of the operation. In other words, this operation is always done from beginning to end, without interruptions\nof another process or operation in the middle of its execution phase.\n\nNot many operations today are atomic. But why do atomic operations matter here? It's because data races\n(which is a type of a race condition) cannot happen on operations that are atomic.\nSo if a particular line in your code performs an atomic operation, then this line will never\nsuffer from a data race problem. Therefore, programmers sometimes use an atomic operation\nto protect themselves from data race problems in their code.\n\nWhen you have an operation that is compiled into just one single assembly instruction, this operation might be atomic,\nbecause it's just one assembly instruction. But this is not guaranteed. This is usually true for old CPU architectures\n(such as `x86`). But nowadays, most assembly instructions in modern CPU architectures are broken down into multiple micro-tasks,\nwhich inherently makes the operation non-atomic, even if it consists of a single assembly instruction.\n\nThe Zig Standard Library offers some atomic functionality in the `std.atomic` module.\nIn this module, you will find a public and generic function called `Value()`. With this function we create an \"atomic object\", which is\na value that contains some native atomic operations, most notably, a `load()` and a `fetchAdd()` operation.\nIf you have experience with multithreading in C++, you probably have recognized this pattern. So yes, this generic\n\"atomic object\" in Zig is essentially identical to the template struct `std::atomic` from the C++ Standard Library.\nIt's important to emphasize that only primitive data types (i.e., the types presented in @sec-primitive-data-types)\nare supported by these atomic operations in Zig.\n\n\n\n\n\n### Data races and race conditions\n\nTo understand why mutexes are used, we need to understand better the problem that they seek\nto solve, which can be summarized into data race problems. A data race problem is a type of a race condition,\nwhich happens when one thread is accessing a particular memory location (i.e., a particular shared object) at the same\ntime that another thread is trying to write/save new data into this same memory location (i.e., the same shared object).\n\nWe can simply define a race condition as any type of bug in your program that is based\non a \"who gets there first\" problem. A data race problem is a type of a race condition, because it occurs when two or more parties\nare trying to read and write into the same memory location at the same time, and, therefore, the end result of this operation\ndepends completely on who gets to this memory location first.\nAs a consequence, a program that has a data race problem will likely produce a different result each time that we execute it.\n\nThus, race conditions produce undefined behaviour and unpredictability because the program produces\na different answer each time a different person gets to the target location before the others.\nAnd, we have no easy way to either predict or control who is getting to this target location first.\nIn other words, each time your program runs, you may get a different answer because a different person,\nfunction, or part of the code finishes its tasks before the others.\n\nAs an example, consider the code snippet exposed below. In this example, we create a global counter\nvariable, and we also create an `increment()` function, whose job is to just increment this global counter\nvariable in a for loop.\n\nSince the for loop iterates 1 hundred thousand times, and, we create two separate threads\nin this code example, what number do you expect to see in the final message printed to `stdout`?\nThe answer should be 2 hundred thousand. Right? Well, in theory, this program was supposed\nto print 2 hundred thousand at the end, but in practice, every time that I execute this program\nI get a different answer.\n\nIn the example exposed below, you can see that this time the end\nresult was 117254, instead of the expected 200000. The second time I executed this program,\nI got the number 108592 as the result. So the end result of this program is varying, but it never gets\nto the expected 200000 that we want.\n\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\n// Global counter variable\nvar counter: usize = 0;\n// Function to increment the counter\nfn increment() void {\n    for (0..100000) |_| {\n        counter += 1;\n    }\n}\n\npub fn main() !void {\n    const thr1 = try Thread.spawn(.{}, increment, .{});\n    const thr2 = try Thread.spawn(.{}, increment, .{});\n    thr1.join();\n    thr2.join();\n    try stdout.print(\"Couter value: {d}\\n\", .{counter});\n}\n```\n:::\n\n\n\n\n```\nCouter value: 117254\n```\n\n\nWhy this is happening? The answer is: because this program contains a data race problem.\nThis program would print the correct number 200000 if and only if the first thread finishes\nits tasks before the second thread starts to execute. But that is very unlikely to happen.\nBecause the process of creating the thread is too fast, and therefore, both threads start to execute roughly\nat the same time. If you change this code to add some nanoseconds of sleep between the first and the second calls to `spawn()`,\nyou will increase the chances of the program producing the \"correct result\".\n\nSo the data race problem happens because both threads are reading and writing to the same\nmemory location at roughly the same time. In this example, each thread is essentially performing\nthree basic operations at each iteration of the for loop, which are:\n\n1. reading the current value of `count`.\n1. incrementing this value by 1.\n1. writing the result back into `count`.\n\nIdeally, a thread B should read the value of `count`, only after the other thread A has finished\nwriting the incremented value back into the `count` object. Therefore, in the ideal scenario, which is demonstrated\nin @tbl-data-race-ideal, the threads should work in sync with each other. But the reality is that these\nthreads are out of sync, and because of that, they suffer from a data race problem, which is demonstrated\nin @tbl-data-race-not.\n\nNotice that, in the data race scenario (@tbl-data-race-not), the read performed by a thread B happens\nbefore the write operation of thread A, and that ultimately leads to wrong results at the end of the program.\nBecause when thread B reads the value of the `count` variable, thread A is still processing\nthe initial value from `count`, and has not yet written the new, incremented value back to `count`. As a result,\nthread B ends up reading the same initial (or \"old\") value from `count` instead of\nthe updated, incremented value that thread A would have written.\n\n\n::: {#tbl-data-race-ideal}\n\n| Thread 1    | Thread 2    | Integer value |\n|-------------|-------------|---------------|\n| read value  |             | 0             |\n| increment   |             | 1             |\n| write value |             | 1             |\n|             | read value  | 1             |\n|             | increment   | 2             |\n|             | write value | 2             |\n\n: An ideal scenario for two threads incrementing the same integer value\n:::\n\n::: {#tbl-data-race-not}\n\n| Thread 1    | Thread 2    | Integer value |\n|-------------|-------------|---------------|\n| read value  |             | 0             |\n|             | read value  | 0             |\n| increment   |             | 1             |\n|             | increment   | 1             |\n| write value |             | 1             |\n|             | write value | 1             |\n\n: A data race scenario when two threads are incrementing the same integer value\n:::\n\n\nIf you think about these diagrams exposed in form of tables, you will notice that they relate back to our discussion of atomic operations\nin @sec-atomic-operation. Remember, atomic operations are operations that the CPU executes\nfrom beginning to end, without interruptions from other threads or processes. So,\nthe scenario exposed in @tbl-data-race-ideal does not suffer from a data race, because\nthe operations performed by thread A are not interrupted in the middle by the operations\nfrom thread B.\n\nIf we also think about the discussion of critical section from @sec-critical-section, we can identify\nthe section that representes the critical section of the program, which is the section that is vulnerable\nto data race conditions. In this example, the critical section of the program is the line where we increment\nthe `counter` variable (`counter += 1`). So, ideally, we want to use a mutex, and lock right before this line, and then\nunlock right after this line.\n\n\n\n\n### Using mutexes in Zig\n\nNow that we know the problem that mutexes seek to solve, we can learn how to use them in Zig.\nMutexes in Zig are available through the `std.Thread.Mutex` struct from the Zig Standard Library.\nIf we take the same code from the previous example, and improve it with mutexes, to solve\nour data race problem, we get the code example below.\n\nNotice that this time, we had to alter the `increment()` function to receive a pointer to\nthe `Mutex` object as input. All that we need to do, to make this program safe against\ndata race problems, is to call the `lock()` method at the beginning of\nthe critical section, and then, call `unlock()` at the end of the critical section.\nNotice that the output of this program is now the correct number of 200000.\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst std = @import(\"std\");\nconst stdout = std.io.getStdOut().writer();\nconst Thread = std.Thread;\nconst Mutex = std.Thread.Mutex;\nvar counter: usize = 0;\nfn increment(mutex: *Mutex) void {\n    for (0..100000) |_| {\n        mutex.lock();\n        counter += 1;\n        mutex.unlock();\n    }\n}\n\npub fn main() !void {\n    var mutex: Mutex = .{};\n    const thr1 = try Thread.spawn(.{}, increment, .{&mutex});\n    const thr2 = try Thread.spawn(.{}, increment, .{&mutex});\n    thr1.join();\n    thr2.join();\n    try stdout.print(\"Couter value: {d}\\n\", .{counter});\n}\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCouter value: 200000\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## Read/Write locks\n\nMutexes are normally used when it's not always safe for two or more threads running the same\npiece of code at the same time. In contrast, read/write locks are normally used in situations\nwhere you have a mixture of scenarios, i.e., there are some pieces of the codebase that are safe\nto run in parallel, and other pieces that are not safe.\n\nFor example, suppose that you have multiple threads that uses the same shared file in the filesystem to store some configurations, or\nstatistics. If two or more threads try to read the data from this same file at the same time, nothing bad happens.\nSo this part of the codebase is perfectly safe to be executed in parallel, with multiple threads reading the same file at the same time.\n\nHowever, if two or more threads try to write data into this same file at the same time, then we cause some race condition\nproblems. So this other part of the codebase is not safe to be executed in parallel.\nMore specifically, a thread might end up writing data in the middle of the data written by the other thread.\nThis process of two or more threads writing to the same location might lead to data corruption.\nThis specific situation is usually called a *torn write*.\n\nThus, what we can extract from this example is that there are certain types of operations that cause\na race condition, but there are also other types of operations that do not cause a race condition problem.\nYou could also say that there are types of operations that are susceptible to race condition problems,\nand there are other types of operations that are not.\n\nA read/write lock is a type of lock that acknowledges the existence of this specific scenario, and you can\nuse this type of lock to control which parts of the codebase are safe to run in parallel and which parts are not.\n\n\n\n### Exclusive lock vs shared lock\n\nTherefore, a read/write lock is a little different from a mutex. Because a mutex is always an *exclusive lock*, meaning that, only\none thread is allowed to execute at all times. With an exclusive lock, the other threads are always \"excluded\",\ni.e., they are always blocked from executing. But in a read/write lock, the other threads might be authorized\nto run at the same time, depending on the type of lock that they acquire.\n\nWe have two types of locks in a read/write lock, which are: an exclusive lock and a shared lock. An exclusive lock works exactly the same\nas a mutex, while a shared lock is a lock that does not block the other threads from running at the same time.\nIn the `pthreads` C library, read/write locks are available through the `pthread_rwlock_t` C struct. With\nthis C struct, you can create:\n\n- a \"write lock\", which corresponds to an exclusive lock.\n- a \"read lock\", which corresponds to a shared lock.\n\nThe terminology might be a little different compared to Zig. But the meaning is still the same.\nTherefore, just remember this relationship, write locks are exclusive locks, while read locks are shared locks.\n\nWhen a thread tries to acquire a read lock (i.e., a shared lock), this thread gets the shared lock\nif and only if another thread does not currently hold a write lock (i.e., an exclusive lock), and also\nif there are no other threads already in the queue,\nwaiting for their turn to acquire a write lock. In other words, the thread in the queue has attempted\nto get a write lock earlier, but this thread was blocked\nbecause there was another thread running that already had a write lock. As a consequence, this thread is in the queue to get a write lock,\nand it's currently waiting for the other thread with a write lock to finish its execution.\n\nWhen a thread tries to acquire a read lock, but it fails in acquiring this read lock, either because there is\na thread with a write lock already running, or because there is a thread in the queue to get a write lock,\nthe execution of this thread is instantly blocked, i.e., paused. This thread will indefinitely attempt to get the\nread lock, and its execution will be unblocked (or unpaused) only after this thread successfully acquires the read lock.\n\nIf you think deeply about this dynamic between read locks versus write locks, you might notice that a read lock is basically a safety mechanism.\nMore specifically, it's a way for us to\nallow a particular thread to run together with the other threads only when it's safe to. In other words, if there is currently\na thread with a write lock running, then it's very likely not safe for the thread that is trying to acquire the read lock to run now.\nAs a consequence, the read lock protects this thread from running into dangerous waters, and patiently waits for the\n\"write lock\" thread to finishes its tasks before it continues.\n\nOn the other hand, if there are only \"read lock\" (i.e., \"shared lock\") threads currently running\n(i.e., not a single \"write lock\" thread currently exists), then it\nis perfectly safe for this thread that is acquiring the read lock to run in parallel with the other\nthreads. As a result, the read lock just\nallows for this thread to run together with the other threads.\n\nThus, by using read locks (shared locks) in conjunction with write locks (exclusive locks), we can control which regions or sections\nof our multithreaded code is safe to have parallelism, and which sections are not safe to have parallelism.\n\n\n\n\n\n### Using read/write locks in Zig\n\nThe Zig Standard Library supports read/write locks through the `std.Thread.RwLock` module.\nIf you want a particular thread to acquire a shared lock (i.e., a read lock), you should\ncall the `lockShared()` method from the `RwLock` object. But, if you want this thread\nto acquire an exclusive lock (i.e., a write lock) instead, then you should call the\n`lock()` method from the `RwLock` object.\n\nAs with mutexes, we also have to unlock the shared or exclusive locks that we acquire through a read/write lock object,\nonce we are at the end of our \"critical section\". If you have acquired an exclusive lock, then, you unlock\nthis exclusive lock by calling the `unlock()` method from the read/write lock object. In contrast,\nif you have acquired a shared lock instead, then, call `unlockShared()` to unlock this shared lock.\n\nAs a simple example, the snippet below creates three separate threads responsible for reading the\ncurrent value in a `counter` object, and it also creates another thread responsible for writing\nnew data into the `counter` object (incrementing it, more specifically).\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nvar counter: u32 = 0;\nfn reader(lock: *RwLock) !void {\n    while (true) {\n        lock.lockShared();\n        const v: u32 = counter;\n        try stdout.print(\"{d}\", .{v});\n        lock.unlockShared();\n        std.time.sleep(2 * std.time.ns_per_s);\n    }\n}\nfn writer(lock: *RwLock) void {\n    while (true) {\n        lock.lock();\n        counter += 1;\n        lock.unlock();\n        std.time.sleep(2 * std.time.ns_per_s);\n    }\n}\n\npub fn main() !void {\n    var lock: RwLock = .{};\n    const thr1 = try Thread.spawn(.{}, reader, .{&lock});\n    const thr2 = try Thread.spawn(.{}, reader, .{&lock});\n    const thr3 = try Thread.spawn(.{}, reader, .{&lock});\n    const wthread = try Thread.spawn(.{}, writer, .{&lock});\n\n    thr1.join();\n    thr2.join();\n    thr3.join();\n    wthread.join();\n}\n```\n:::\n\n\n\n\n\n## Yielding a thread\n\nThe `Thread` struct supports yielding through the `yield()` method.\nYielding a thread means that the execution of the thread is temporarily stopped,\nand it moves to the end of the priority queue managed by the scheduler of\nyour operating system.\n\nThat is, when you yield a thread, you are essentially saying the following to your OS:\n\"Hey! Could you please stop executing this thread for now, and comeback to continue it later?\".\nYou could also interpret this yield operation as: \"Could you please deprioritize this thread,\nto focus on doing other things instead?\".\nSo this yield operation is also a way for you\nto stop a particular thread, so that you can work and prioritize other threads instead.\n\nIt's important to say that, yielding a thread is a \"not so common\" thread operation these days.\nIn other words, not many programmers use yielding in production, simply because it's hard to use\nthis operation and make it work properly, and also, there\nare better alternatives. Most programmers prefer to use `join()` instead.\nIn fact, most of the time, when you see someone using this \"yield\" operation in some code example,\nthey are usually doing so to help debug race conditions in their applications.\nThat is, this \"yield\" operation is mostly used as a debug tool nowadays.\n\nAnyway, if you want to yield a thread, just call the `yield()` method from it, like this:\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nthread.yield();\n```\n:::\n\n\n\n\n\n\n\n\n\n## Common problems in threads\n\n\n\n### Deadlocks\n\nA deadlock occurs when two or more threads are blocked forever,\nwaiting for each other to release a resource. This usually happens when multiple locks are involved,\nand the order of acquiring them is not well managed.\n\nThe code example below demonstrates a deadlock situation. We have two different threads that execute\ntwo different functions (`work1()` and `work2()`) in this example. And we also have two separate\nmutexes. If you compile and run this code example, you will notice that the program just runs indefinitely,\nwithout ending.\n\nWhen we look into the first thread, which executes the `work1()` function, we can\nnotice that this function acquires the `mut1` lock first. Because this is the first operation\nthat is executed inside this thread, which is the first thread created in the program.\nAfter that, the function sleeps for 1 second, to\nsimulate some type of work, and then, the function tries to acquire the `mut2` lock.\n\nOn the other hand, when we look into the second thread, which executes the `work2()` function,\nwe can see that this function acquires the `mut2` lock first. Because when this thread gets created and it tries\nto acquire this `mut2` lock, the first thread is still sleeping on that \"sleep 1 second\" line.\nAfter acquiring `mut2`, the `work2()` function also sleeps for 1 second, to\nsimulate some type of work, and then the function tries to acquire the `mut1` lock.\n\nThis creates a deadlock situation, because after the \"sleep for 1 second\" line in both threads,\nthread 1 is trying to acquire the `mut2` lock, but this lock is currently being used by thread 2.\nHowever, at this moment, thread 2 is also trying to acquire the `mut1` lock, which is currently\nbeing used by thread 1. Therefore, both threads end up waiting for ever. Waiting for their peer to\nfree the lock that they want to acquire.\n\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nvar mut1: Mutex = .{}; var mut2: Mutex = .{};\nfn work1() !void {\n    mut1.lock();\n    std.time.sleep(1 * std.time.ns_per_s);\n    mut2.lock();\n    _ = try stdout.write(\"Doing some work 1\\n\");\n    mut2.unlock(); mut1.unlock();\n}\n\nfn work2() !void {\n    mut2.lock();\n    std.time.sleep(1 * std.time.ns_per_s);\n    mut1.lock();\n    _ = try stdout.write(\"Doing some work 1\\n\");\n    mut1.unlock(); mut2.unlock();\n}\n\npub fn main() !void {\n    const thr1 = try Thread.spawn(.{}, work1, .{});\n    const thr2 = try Thread.spawn(.{}, work2, .{});\n    thr1.join();\n    thr2.join();\n}\n```\n:::\n\n\n\n\n\n### Not calling `join()` or `detach()` {#sec-not-call-join-detach}\n\nWhen you do not call either `join()` or `detach()` over a thread, then this thread becomes a \"zombie thread\",\nbecause it does not have a clear \"return point\".\nYou could also interpret this as: \"nobody is properly responsible for managing the thread\".\nWhen we don't establish if a thread is either *joinable* or *detached*,\nnobody becomes responsible for dealing with the return value of this thread, and also,\nnobody becomes responsible for clearing (or freeing) the resources associated with this thread.\n\nYou don't want to be in this situation, so remember to always use `join()` or `detach()`\non the threads that you create. When you don't use one of these methods, we lose\ncontrol over the thread, and its resources are never freed\n(i.e., you have leaked resources in the system).\n\n\n### Cancelling or killing a particular thread\n\nWhen we think about the `pthreads` C library, there is a possible way to asynchronously kill or cancel\na thread, which is by sending a `SIGTERM` signal to the thread through the `pthread_kill()` function.\nBut canceling a thread like this is bad. It's dangerously bad. As a consequence, the Zig implementation\nof threads does not have a similar function, or, a similar way to asynchronously cancel or kill\na thread.\n\nTherefore, if you want to cancel a thread in the middle of its execution in Zig,\nthen one good strategy that you can take is to use control flow in conjunction with `join()`.\nMore specifically, you can design your thread around a while loop that is constantly\nchecking if the thread should continue running.\nIf it's time to cancel the thread, we could make the while loop break, and join the thread with the main thread\nby calling `join()`.\n\nThe code example below demonstrates to some extent this strategy.\nHere, we are using control flow to break the while loop, and exit the thread earlier than\nwhat we have initially planned to. This example also demonstrates how can we use\natomic objects in Zig with the `Value()` generic function that we have mentioned in @sec-atomic-operation.\n\n\n\n\n\n::: {.cell}\n\n```{.zig .cell-code}\nconst std = @import(\"std\");\nconst Thread = std.Thread;\nconst stdout = std.io.getStdOut().writer();\nvar running = std.atomic.Value(bool).init(true);\nvar counter: u64 = 0;\nfn do_more_work() void {\n    std.time.sleep(2 * std.time.ns_per_s);\n}\nfn work() !void {\n    while (running.load(.monotonic)) {\n        for (0..10000) |_| { counter += 1; }\n        if (counter < 15000) {\n            _ = try stdout.write(\n                \"Time to cancel the thread.\\n\"\n            );\n            running.store(false, .monotonic);\n        } else {\n            _ = try stdout.write(\"Time to do more work.\\n\");\n            do_more_work();\n            running.store(false, .monotonic);\n        }\n    }\n}\n\npub fn main() !void {\n    const thread = try Thread.spawn(.{}, work, .{});\n    thread.join();\n}\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime to cancel the thread.\n```\n\n\n:::\n:::\n",
    "supporting": [
      "14-threads_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}